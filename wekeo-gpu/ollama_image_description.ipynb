{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/tools/frameworks/-/raw/main/img/Standard_banner.png' align='right' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#138D75\">**Copernicus EUMETSAT**</font> <br>\n",
    "**Copyright:** 2025 EUMETSAT <br>\n",
    "**License:** MIT <br>\n",
    "**Authors:** Hugues Sassier (Thales Alenia Space), Anna-Lena Erdmann (EUMETSAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "  <div style=\"width:100%\">\n",
    "    <div style=\"float:left\"><a href=\"https://jupyterhub.prod.wekeo2.eu/hub/user-redirect/lab/tree/public/wekeo4data/wekeo-gpu/ollama_image_description.ipynb\"><img src=\"https://img.shields.io/badge/launch-WEKEO-1a4696.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAMCAMAAACKnBfWAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAABwlBMVEUAYbdepNtBc7VCdLhNhMQkXKpsn9RWltFZl9VlpNI3aK9fpNtMd7ZznM8pg8o4WJc2TZFHY6MmQYptiLxnqt5hptxfpNtfpNtfpNtgpdtip9x2uullqN1fpNtdo9pepdtanNQ9a65Bc7dCeb5nqt5fpNslUpssW6RMgcBTjstWldJhptxdo9phoNdkpdtfpNtoqNxepNplo9hepNthnNNfpdtelM1hptxSgL5dj8hoqt5fpNsjRYgjPog8WqBHa61TfLplqN1fpdtfpdxam9IyTIw2TZFGYaBoqt5hptxfpdtepNpfpNtgpdtjp9x5vu5do9pHgb5co9pepNtHgbwWN4AVP5AhU6JLgL9dl89epNphpttgpdtipttnqd1ho9glSIkWM3snSokeTJdHda9cksphndRmqNyTweaWxOePv+WfyemAs91QYphFUmpuclhKYHRwh5V8pNNdls6iyuq72fCNvuWIueCIlbtTW2hwcFNTY2lmfpSBpNJZj8lsq91xr99nqdx4suBrqds7V5AlNm85S20lRIJVcZxZgr1Uh8NcotpbotpdpNtGfrcTKnAOKXsYOYo4XKRJcbNHfbf////t/CgnAAAAUHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAAAByF2PfyxGAJLcH+9pQvJgsav/7s57hBf/3gQdO69vH18dK4fN0+GLz96uSzPCq99JArIgkagNTz7r9bB4DtcFsAAAABYktHRJUIYHqDAAAAB3RJTUUH5wIKESIRAg6dCwAAALtJREFUCNdjYGAUERUTl5CUkmZiZmFlY2CUkZULAAJ5BUUlZXYOBhXVADAIVFPX0NTiZNDWAXKCgkNCw8IjInX1GPSjomNi4+ITEpOSU1LTDBgM0zMys7JzcvPyCwqLio0YjKNKSstKyisqq6prautMGEwD6hsam5pbWtvaOzq7zBjMdbq7g3p6+/onTJw02cKSwcoaYt8UG1s7ewcuBm5HJ7B7nF1c3dx5eBn4uD08vbx9fP38+QUEhYQB5Z40RP8+e1wAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjMtMDItMTBUMTc6MzM6NDUrMDA6MDCCLR1xAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIzLTAyLTEwVDE3OjI2OjU5KzAwOjAw393bowAAAABJRU5ErkJggg==\" alt=\"Open in WEkEO\"></a></div>\n",
    "    <div style=\"float:left\"><p>&emsp;</p></div>\n",
    "  </div>    \n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3> Use Cases for the WEkEO Workspace GPUs </h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b>PREREQUISITES </b>\n",
    "    \n",
    "This notebook has the following prerequisites:\n",
    "  - **<a href=\"https://my.wekeo.eu/user-registration\" target=\"_blank\">A WEkEO account</a>**\n",
    "  - **Execution of the notebook under the WEkEO Workspace <a href=\"https://help.wekeo.eu/en/articles/7945473-which-are-the-computing-resources-of-the-wekeo-jupyterhub\" target=\"_blank\"> Machine Learning (GPU) server</a>**\n",
    "  - minimum of **10 GB of free storage space** on your WEkEO Workspace/device\n",
    "  \n",
    "\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama image description using the WEkEO Workspace GPUs\n",
    "\n",
    "### Learning outcomes\n",
    "\n",
    "At the end of this notebook you will know;\n",
    "\n",
    "* how to set up a ollama server\n",
    "* use the WEkEO Workspace GPUs to run the llava 7B multi-modal model \n",
    "* how to use the model to generate image descriptions of EO data\n",
    "\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook introduces Ollama, a lightweight framework for running large language and vision models locally, and demonstrates the use of LLaVA 7B, a vision-language model designed to interpret both images and text. While LLaVA is commonly used for tasks like visual question answering or image captioning, this notebook explores its potential for analyzing satellite imagery and Earth Observation (EO) data. The example  acts as a demonstrator for leveraging GPU resources provided by WEkEO to run light-weight LLMs and multi-modal models efficiently on the Jupyter Hub.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Contents <a id='totop'></a>\n",
    "\n",
    "</div>\n",
    "\n",
    " 0. [Check Available Storage Space](#section00)   \n",
    " 1. [Set up the Ollama Server](#section0)\n",
    " 2. [Pull the Language Model](#section1)\n",
    " 3. [Generate Image descriptions of EO data](#section2)\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 0. <a id='section00'></a>Check Available Storage Space\n",
    "[Back to top](#totop)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prerequisite of this notebook is the availability of 10 GB of free storage space. The cell below checks if you have enough free storage space in your WEkEO JupyterHub to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Path '/home/jovyan' does not exist. Skipping disk space check.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to check\n",
    "path = \"/home/jovyan\"\n",
    "\n",
    "# Only perform the check if the path exists\n",
    "if os.path.exists(path):\n",
    "    # Get disk usage statistics\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    free_gb = free / (1024 ** 3)\n",
    "\n",
    "    # Raise error if less than 10 GB are free\n",
    "    if free_gb < 10:\n",
    "        raise RuntimeError(\"❌ Please free some space. To execute this Notebook you need at least 10 GB of storage space available in your workspace.\")\n",
    "    else:\n",
    "        print(f\"✅ Disk check passed: {free_gb:.2f} GB free.\")\n",
    "else:\n",
    "    print(f\"⚠️ Path '{path}' does not exist. Skipping disk space check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 1. <a id='section0'></a>Set up the Ollama Server\n",
    "[Back to top](#totop)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To use the LLaVA 7B model, we first need to install the **Ollama server**, which allows you to run large multimodal models locally or on a cloud GPU. \n",
    "\n",
    "Run the following commands to install Ollama on a Linux system:\n",
    "\n",
    "```bash\n",
    "curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
    "tar -C ~/.local -xzf ollama-linux-amd64.tgz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Note:** The download is approximately **1.6 GB**, so make sure you have sufficient disk space and a stable internet connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install Ollama directly from the notebook cell using `!`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1621M  100 1621M    0     0   182M      0  0:00:08  0:00:08 --:--:--  209M\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf ollama-linux-amd64.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can start the ollama server either in the terminal, or through the notebook\n",
    "\n",
    "To start the ollama server in the terminal of a linux system, run the following commands: \n",
    "\n",
    "\n",
    "```bash\n",
    "ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to start it inside the Jupyter Notebooks, however, **Jupyter Notebooks do not support background processes**. So it is essential to execute the ´ollama serve´ command in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama serve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executed in the terminal, the output should state, that the GPUs are detected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/img-description-ollamaserve.png\" alt=\"Sentinel image description\" style=\"width:90%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to install the python package ollama to run the next parts of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 2. <a id='section1'></a>Pull the Language Model\n",
    "[Back to top](#totop)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we decide on the model we want to use for describing the image. In this example, we have chosen the [llava-7b](https://ollama.com/library/llava:7b) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 170370233dd5: 100% ▕██████████████████▏ 4.1 GB                         \u001b[K\n",
      "pulling 72d6f08a42f6: 100% ▕██████████████████▏ 624 MB                         \u001b[K\n",
      "pulling 43070e2d4e53: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling c43332387573: 100% ▕██████████████████▏   67 B                         \u001b[K\n",
      "pulling ed11eda7790d: 100% ▕██████████████████▏   30 B                         \u001b[K\n",
      "pulling 7c658f9561e5: 100% ▕██████████████████▏  564 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llava:7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Explore\n",
    "\n",
    "You can pull many different models from the ollama server. The llava-7b is just one example. As an exercise you can browse through the [model repository of ollama](https://ollama.com/search) and try out pulling different models. \n",
    "\n",
    "Be reminded, that the GPU on the WEkEO Workspace has approximately 7 GB of Memory. The models have to fit into memory in order to be used. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 3. <a id='section2'></a>Generate Image descriptions of EO data\n",
    "[Back to top](#totop)\n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to let the model generate an image description for a satellite image. The satellite image used is from the Copernicus Senitnel-2 Satellite. You can learn more about the data in the [WEkEO Data Catalog](https://wekeo.copernicus.eu/data?view=dataset&dataset=EO:ESA:DAT:SENTINEL-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/img-description-sentinel.png\" alt=\"Sentinel image description\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image you've provided appears to be a satellite or aerial view of an area, likely a rural landscape given the presence of fields and what looks like a farmland. The land is divided into sections that are colored differently—orange and green patches stand out. There is text at the bottom of the image that reads \"Satellite View Of Some Place.\" Additionally, there's a small inset map with a red outline of an area within a larger yellow-brown rectangle, suggesting this view might be a section of a larger map or satellite image. The overall quality and resolution are low, indicating that the image may not be high-resolution and could be for illustrative purposes rather than precise navigation or geographic analysis. "
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "\tmodel=\"llava:7b\",\n",
    "\tmessages=[\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': 'Describe this image:',\n",
    "\t\t\t'images': ['img/img-description-sentinel.png']\n",
    "\t\t}\n",
    "\t],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Please note:** Language models, including LLaVA, may sometimes produce **hallucinations**—outputs that sound plausible but are factually incorrect. Always double-check the generated results, especially when used in analytical or operational contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You have just successfully generated an image description using a language model—powered by **Ollama** and the **LLaVA 7B model**—running on **WEkEO GPUs**. This demonstrates how vision-language models can also be applied to **satellite imagery** and **Earth Observation (EO) data**.\n",
    "\n",
    "To learn more about using **GPUs in the WEkEO JupyterLab Workspace**, visit the [WEkEO Help Centre article on GPU usage](https://help.wekeo.eu/en/articles/7945473-which-are-the-computing-resources-of-the-wekeo-jupyterhub)\n"
   ]
  }
 ],
 "metadata": {
  "author": "Anna-Lena Erdmann",
  "deployment": {
   "wekeo": {
    "git": {
     "link": "https://github.com/wekeo/wekeo4data/blob/main/wekeo-gpu/ollama_image_description.ipynb",
     "service_contact": "support@wekeo.eu",
     "service_provider": "EUMETSAT"
    },
    "url": {
     "link": "https://jupyterhub.prod.wekeo2.eu/hub/user-redirect/lab/tree/public/wekeo4data/wekeo-gpu/ollama_image_description.ipynb",
     "service_contact": "support@wekeo.eu",
     "service_provider": "EUMETSAT"
    }
   }
  },
  "description": "This Jupyter Notebook demonstrated how to use a vision-language model from ollama with the WEkEO GPUs",
  "image": "img/thumbs/image-description-ollama.png",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "tags": {
   "service": [
    "EUMETSAT"
   ],
   "subtheme": [],
   "theme": "Tools"
  },
  "title": "Use a Vision Language Model on the WEkEO Workspace GPUs",
  "vscode": {
   "interpreter": {
    "hash": "69a843eb2806097c81d46a99bb39f1f0d8214ba0939f50116f16a105a7e284d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
