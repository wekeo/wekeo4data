{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/tools/frameworks/-/raw/main/img/Standard_banner.png' align='right' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#138D75\">**Copernicus EUMETSAT**</font> <br>\n",
    "**Copyright:** 2025 EUMETSAT <br>\n",
    "**License:** MIT <br>\n",
    "**Author:** Anna-Lena Erdmann (EUMETSAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "  <div style=\"width:100%\">\n",
    "    <div style=\"float:left\"><a href=\"https://jupyterhub.prod.wekeo2.eu/hub/user-redirect/lab/tree/public/wekeo4data/wekeo-gpu/01_Introduction_to_the_WEkEO_Workspace_GPUs.ipynb\"><img src=\"https://img.shields.io/badge/launch-WEKEO-1a4696.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAMCAMAAACKnBfWAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAABwlBMVEUAYbdepNtBc7VCdLhNhMQkXKpsn9RWltFZl9VlpNI3aK9fpNtMd7ZznM8pg8o4WJc2TZFHY6MmQYptiLxnqt5hptxfpNtfpNtfpNtgpdtip9x2uullqN1fpNtdo9pepdtanNQ9a65Bc7dCeb5nqt5fpNslUpssW6RMgcBTjstWldJhptxdo9phoNdkpdtfpNtoqNxepNplo9hepNthnNNfpdtelM1hptxSgL5dj8hoqt5fpNsjRYgjPog8WqBHa61TfLplqN1fpdtfpdxam9IyTIw2TZFGYaBoqt5hptxfpdtepNpfpNtgpdtjp9x5vu5do9pHgb5co9pepNtHgbwWN4AVP5AhU6JLgL9dl89epNphpttgpdtipttnqd1ho9glSIkWM3snSokeTJdHda9cksphndRmqNyTweaWxOePv+WfyemAs91QYphFUmpuclhKYHRwh5V8pNNdls6iyuq72fCNvuWIueCIlbtTW2hwcFNTY2lmfpSBpNJZj8lsq91xr99nqdx4suBrqds7V5AlNm85S20lRIJVcZxZgr1Uh8NcotpbotpdpNtGfrcTKnAOKXsYOYo4XKRJcbNHfbf////t/CgnAAAAUHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAAAByF2PfyxGAJLcH+9pQvJgsav/7s57hBf/3gQdO69vH18dK4fN0+GLz96uSzPCq99JArIgkagNTz7r9bB4DtcFsAAAABYktHRJUIYHqDAAAAB3RJTUUH5wIKESIRAg6dCwAAALtJREFUCNdjYGAUERUTl5CUkmZiZmFlY2CUkZULAAJ5BUUlZXYOBhXVADAIVFPX0NTiZNDWAXKCgkNCw8IjInX1GPSjomNi4+ITEpOSU1LTDBgM0zMys7JzcvPyCwqLio0YjKNKSstKyisqq6prautMGEwD6hsam5pbWtvaOzq7zBjMdbq7g3p6+/onTJw02cKSwcoaYt8UG1s7ewcuBm5HJ7B7nF1c3dx5eBn4uD08vbx9fP38+QUEhYQB5Z40RP8+e1wAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjMtMDItMTBUMTc6MzM6NDUrMDA6MDCCLR1xAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIzLTAyLTEwVDE3OjI2OjU5KzAwOjAw393bowAAAABJRU5ErkJggg==\" alt=\"Open in WEkEO\"></a></div>\n",
    "    <div style=\"float:left\"><p>&emsp;</p></div>\n",
    "  </div>    \n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3> Getting Started with the WEkEO Workspace GPUs </h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b>PREREQUISITES </b>\n",
    "    \n",
    "This notebook has the following prerequisites:\n",
    "  - **<a href=\"https://my.wekeo.eu/user-registration\" target=\"_blank\">A WEkEO account</a>**\n",
    "  - **Execution of the notebook under the WEkEO Workspace <a href=\"https://help.wekeo.eu/en/articles/7945473-which-are-the-computing-resources-of-the-wekeo-jupyterhub\" target=\"_blank\"> Machine Learning (GPU) server</a>**\n",
    "  \n",
    "\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the WEkEO Workspace GPUs\n",
    "\n",
    "### Learning outcomes\n",
    "\n",
    "At the end of this notebook you will know:\n",
    "\n",
    "* how to access the WEkEO Workspace GPU Server\n",
    "* how to monitor GPU usage \n",
    "* how to compare runtimes of GPUs and CPUs \n",
    "\n",
    "\n",
    "### Outline\n",
    "\n",
    "In this notebook, you will learn how to access and use the GPU resources available in the WEkEO Workspace to accelerate machine learning tasks. It is the starting point to get familiarized with GPUs and lays the basis for upcoming Machine Learning use cases. You will use the GPU with a simple tensor multiplication, monitor GPU usage during the calculations, and compare the performance of CPU and GPU runtimes. This example provides a hands-on introduction to working with GPU-accelerated deep learning in the WEkEO Workspace. \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Contents <a id='totop'></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "1. [Accessing the WEkEO Workspace GPU Environment](#section0)  \n",
    "2. [Monitoring GPU Usage](#section1)   \n",
    "3. [Comparing GPU and CPU Runtimes](#section2)  \n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 1. <a id='section0'></a>Accessing the WEkEO Workspace GPU Environment\n",
    "[Back to top](#totop)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with GPU-accelerated machine learning in the WEkEO platform, follow these steps:\n",
    "\n",
    "1. Log in to your [WEkEO user account](https://www.wekeo.eu/) and go to your **personal dashboard**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/wekeo-landing-page.png\" alt=\"Landing Page\" style=\"width:60%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the dashboard, navigate to the **\"Workspace\"** tab and launch the **WEkEO JupyterLab environment**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/wekeo-personal-dashboard.png\" alt=\"WEkEO Personal Dashboard\" style=\"width:60%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Once prompted, select the server type. Choose **\"Machine Learning (GPUs)\"** to request a server with GPU resources.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/gpu-server.png\" alt=\"GPU Server Selection\" style=\"width:60%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. After the environment loads, open a **Terminal** from the JupyterLab launcher.\n",
    "\n",
    "To verify that the GPU is available, you can run the following command in the terminal:\n",
    "\n",
    "`nvidia-smi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/nvidia-smi.png\" alt=\"GPU Server Selection\" style=\"width:60%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that **6144 MB of GPU Memory** are available in your JupyterHub environment. You can run the `nvidia-smi` command anytime you want to check if you are in the GPU server or on the non-GPU Earth Observation / Machine LEarning Server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 2. <a id='section1'></a>Monitor GPU Usage\n",
    "[Back to top](#totop)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on a GPU-enabled machine, it's useful to monitor your GPU's memory usage and activity to make sure your code is running efficiently. One of the most user-friendly tools for this is `nvtop`.\n",
    "\n",
    "### What is `nvtop`?\n",
    "\n",
    "`nvtop` (NVIDIA TOP) is an interactive command-line utility similar to `htop`, but specifically designed for monitoring NVIDIA GPUs. It displays real-time information on:\n",
    "- GPU memory usage\n",
    "- Utilization percentage\n",
    "- Active processes using the GPU\n",
    "- Power consumption (if available)\n",
    "\n",
    "It is ideal for checking whether your training process is actively using the GPU and how much memory is being consumed.\n",
    "\n",
    "\n",
    "### Install nvtop to the environment\n",
    "\n",
    "You can run in the terminal: \n",
    "\n",
    "```bash\n",
    "    conda install -c conda-forge nvtop -y\n",
    "```\n",
    "\n",
    "Or execute the code cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge -q nvtop -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This installs the GPU monitoring tool in your Conda environment. \n",
    "\n",
    "Note: this will not be persistent if you install it in the Python3 environment of the JupyterHub. Create a new custom environment to have the tool persistently in your environment. \n",
    "\n",
    "\n",
    "### Run `nvtop`\n",
    "\n",
    "Once installed, you can monitor your GPU in real time by opening a **terminal** in JupyterLab and running:\n",
    "\n",
    "```bash\n",
    "nvtop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monitoring tool will open in the terminal. You see two lines: one for the GPU memory and one for the GPU utilization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/nvtop.png\" alt=\"nvtop\" style=\"width:60%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a matrix calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the GPU monitoring tool in action, we will start a simple tensor multiplication in pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings (e.g., related to data type conversions or GPU init)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set the size of the matrix\n",
    "matrix_size = 10000\n",
    "\n",
    "# Generate two random matrices\n",
    "a = torch.randn(matrix_size, matrix_size)\n",
    "b = torch.randn(matrix_size, matrix_size)\n",
    "\n",
    "# Move the matrices to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a_gpu = a.to(device)\n",
    "b_gpu = b.to(device)\n",
    "\n",
    "\n",
    "# Measure GPU time \n",
    "_ = torch.matmul(a_gpu, b_gpu)  \n",
    "torch.cuda.synchronize()        \n",
    "result_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "torch.cuda.synchronize()       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing the matrix multiplication, we can see the level of GPU memory and GPU utilization changing in `nvtop`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/matrix-multiplication.png\" alt=\"nvtop\" style=\"width:60%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that even after the calculation is finished (blue line), the GPU memory is still occupied. To release the GPU memory, we can clear the GPU cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 3. <a id='section2'></a>Comparing GPU and CPU Runtimes\n",
    "[Back to top](#totop)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key advantages of using the WEkEO Workspace GPU server is the significant speed-up it can offer for large-scale numerical computations. To demonstrate this, we'll compare how long it takes to perform a large matrix multiplication using the CPU versus the GPU.\n",
    "\n",
    "This type of operation is common in machine learning tasks like neural network training, where many matrix operations are performed in sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the product of two large square matrices (3000 Ã— 3000) using PyTorch, once on the CPU and once on the GPU. It is very similar to the code above, except that we run the matrix multiplication on both, the GPU and the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.4448 seconds\n",
      "GPU time: 0.0020 seconds (if GPU available)\n"
     ]
    }
   ],
   "source": [
    "# Set the matrix size\n",
    "matrix_size = 3000\n",
    "\n",
    "# Create random matrices on the CPU\n",
    "a_cpu = torch.randn(matrix_size, matrix_size)\n",
    "b_cpu = torch.randn(matrix_size, matrix_size)\n",
    "\n",
    "# Also move the matrices to the GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a_gpu = a_cpu.to(device)\n",
    "b_gpu = b_cpu.to(device)\n",
    "\n",
    "# Measure CPU computation time\n",
    "start_cpu = time.time()\n",
    "result_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "end_cpu = time.time()\n",
    "cpu_time = end_cpu - start_cpu\n",
    "\n",
    "# Warm-up and measure GPU computation time\n",
    "\n",
    "start_gpu = time.time()\n",
    "result_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "torch.cuda.synchronize()\n",
    "end_gpu = time.time()\n",
    "gpu_time = end_gpu - start_gpu\n",
    "\n",
    "# Print results\n",
    "print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "print(f\"GPU time: {gpu_time:.4f} seconds (if GPU available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the results, that the GPU is performing the martix calculation **40x faster** than the CPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You have successfully completed the first notebook about the WEkEO WOrkspace GPUs! You are now equipped with all the tools you require to get started with GPUs for you EO projects. \n",
    "\n",
    "Do you want to find out what use cases you can do next with the WEkEO Workspace GPUs? Ceck out the [notebook on vision-language models](https://jupyterhub.prod.wekeo2.eu/hub/user-redirect/lab/tree/public/wekeo4data/wekeo-gpu/ollama_image_description.ipynb) and how to generate satellite data descriptions with them. "
   ]
  }
 ],
 "metadata": {
  "author": "Anna-Lena Erdmann",
  "deployment": {
   "wekeo": {
    "git": {
     "link": "https://github.com/wekeo/wekeo4data/blob/main/wekeo-gpu/01_Introduction_to_the_WEkEO_Workspace_GPUs.ipynb",
     "service_contact": "support@wekeo.eu",
     "service_provider": "EUMETSAT"
    },
    "url": {
     "link": "https://jupyterhub.prod.wekeo2.eu/hub/user-redirect/lab/tree/public/wekeo4data/wekeo-gpu/01_Introduction_to_the_WEkEO_Workspace_GPUs.ipynb",
     "service_contact": "support@wekeo.eu",
     "service_provider": "EUMETSAT"
    }
   }
  },
  "description": "This Jupyter Notebook introduced the WEkEO Workspace GPUs and useful tools to work with GPUs",
  "image": "img/thumbs/intro-gpu.png",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "tags": {
   "service": [
    "EUMETSAT"
   ],
   "subtheme": [],
   "theme": "Tools"
  },
  "title": "Introduction to the WEkEO Workspace GPUs",
  "vscode": {
   "interpreter": {
    "hash": "69a843eb2806097c81d46a99bb39f1f0d8214ba0939f50116f16a105a7e284d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
